{
    "title": "Dell EMC\u2019s Powerhouse Machine Learning Server: up to 10 GPUs",
    "author": "Doug Black",
    "date": "2019-04-30 00:00:00",
    "text": "Dell EMC has launched a thoroughbred machine learning server for the data center and at the edge, its most powerful in the company\u2019s line of AI servers, with four, eight or 10 Nvidia Tesla V100 GPUs (14 single-precision teraflops or 112 mixed-precision \"AI\" teraflops per processor) and up to eight NVMe local storage drives.In short: a lot of data, a lot of power.Announced this morning on day two at Dell Technologies World, the Dell EMC DSS 8440 is a two-socket, 4U server designed for ML applications and other demanding workloads. The server isn\u2019t for machine learning rookies \u2013 it\u2019s built for organizations whose AI journeys have proceeded to the point of using more sophisticated, complex models utilizing greater amounts of data for which faster training, and quicker training iterations, is critical.\u201cMost people starting in AI don\u2019t need eight GPUs,\u201d said Dell EMC Senior Quality Engineer David Hillenmeyer at a pre-announcement event yesterday in Las Vegas. \u201cIt\u2019s an extremely powerful box, but it\u2019s not a starting point. Depending on where customers are on their journey, if they\u2019re trying to understand where machine learning will help them, there are a variety of products, servers and accelerators that a customer can deploy.\u201dSuch as Dell EMC\u2019s 740 and 7425 servers, which support up to three GPUs, and the 4140, supporting up to four accelerators.Hillenmeyer said the DSS 8440 is designed to deliver machine learning processing capability driven by development of cloud-native applications that process massive amounts of data. Beyond high performance processing capability, the server has high-speed I/O, with four Micro-Semi 96-lane PCIe switches and eight x4 NVMe PCIe slots.Performance does not scale precisely linearly as the number of GPUs in the server increases from four to 10, Hillenmeyer said, though it comes close. He shared data showing that for image recognition workloads, using a single Tesla V100, training can take roughly 1000 minutes while with 8 V100s takes about 180 minutes; for language translation, training with one v100 requires 275 minutes while with eight Nvidia processors the time is cut to 50 minutes, and so forth.The server also supports the most powerful Intel Xeon Scalable CPUs with up to 24 cores per processor.For storage, the DSS 8440 can support up to 10 2.5 inch devices, or 32 terabytes of NVMe storage, \u201cso you can have a large sample of training data, and fast access to it, to reduce your training times,\u201d Hillenmeyer said.The company said the DSS 8440 has an open architecture, based on industry-standard PCIe fabric, allowing for customization of accelerators, storage options and network cards. \u201cThis platform is designed to be open to support new technologies coming in this area,\u201d said Hillenmeyer, including the Graphcore machine learning IPU processor, developed jointly with Dell EMC, due for release during the second half of this year, along with FPGAs, he said.Hillenmeyer said the server can be used in the core data center or out at the edge for workloads requiring low latency. \u201cWhen we\u2019re talking about generation of a large amount of data we can take this box out to where the data is being generated, as opposed to having to transport all of the data back to the core.\u201dIt is set to ship this quarter.",
    "pic_urls": [
        "https://2pggys3b7fd63bfvol1w51zt-wpengine.netdna-ssl.com/wp-content/uploads/2019/04/Dell-EMC-DSS-8440-ML-Server-10-Nvidia-GPUs-0419-370x290.jpg",
        "https://2pggys3b7fd63bfvol1w51zt-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Dell-EMC-logo-0818.jpg"
    ]
}